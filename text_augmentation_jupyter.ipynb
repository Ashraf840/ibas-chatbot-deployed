{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e69aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zubair/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zubair/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/ibas_project\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "711cd7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word.isalpha()]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) > 0:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540c4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char.isalpha()])\n",
    "            if synonym != word:\n",
    "                synonyms.append(synonym)\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98fdd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentence(sentence, n=10):\n",
    "    augmented_sentences = [sentence]\n",
    "    words = word_tokenize(sentence)\n",
    "    for _ in range(n-1):\n",
    "        augmented_sentences.append(synonym_replacement(words, 1))\n",
    "    \n",
    "    # Shuffle the entire set of augmented sentences\n",
    "    random.shuffle(augmented_sentences)\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a5a9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_data(input_file, output_file):\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    augmented_data = {'bangla_ques': [], 'transliterated_ques': [], 'english_ques': []}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        bangla_question = row['bangla_ques']\n",
    "        transliterated_question = row['transliterated_ques']\n",
    "        english_question = row['english_ques']\n",
    "\n",
    "        augmented_bangla = augment_sentence(bangla_question, 10)\n",
    "        augmented_transliterated = augment_sentence(transliterated_question, 10)\n",
    "        augmented_english = augment_sentence(english_question, 10)\n",
    "\n",
    "        augmented_data['bangla_ques'].extend(augmented_bangla)\n",
    "        augmented_data['transliterated_ques'].extend(augmented_transliterated)\n",
    "        augmented_data['english_ques'].extend(augmented_english)\n",
    "\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    augmented_df.to_excel(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601e4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = \"/home/zubair/workstation_2/source/Final-updated-dataset.xlsx\"  # Change this to the path of your input file\n",
    "    output_file = \"/home/zubair/workstation_2/source/Final-updated-augmented-dataset.xlsx\"  # Change this to the desired output file name\n",
    "    augment_data(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d39783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zubair/anaconda3/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Collecting bnaug\n",
      "  Downloading bnaug-1.1.2-py3-none-any.whl (4.8 kB)\n",
      "Collecting bnlp-toolkit==3.3.2 (from bnaug)\n",
      "  Downloading bnlp_toolkit-3.3.2-py3-none-any.whl (23 kB)\n",
      "Collecting transformers==4.24.0 (from bnaug)\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (0.1.95)\n",
      "Requirement already satisfied: gensim in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (4.1.2)\n",
      "Requirement already satisfied: nltk in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (3.7)\n",
      "Requirement already satisfied: numpy in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (1.21.5)\n",
      "Requirement already satisfied: scipy in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (1.7.3)\n",
      "Requirement already satisfied: sklearn-crfsuite in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (0.3.6)\n",
      "Requirement already satisfied: tqdm in /home/zubair/anaconda3/lib/python3.9/site-packages (from bnlp-toolkit==3.3.2->bnaug) (4.65.0)\n",
      "Collecting ftfy (from bnlp-toolkit==3.3.2->bnaug)\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m336.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting emoji==1.7.0 (from bnlp-toolkit==3.3.2->bnaug)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (2022.3.15)\n",
      "Requirement already satisfied: requests in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/zubair/anaconda3/lib/python3.9/site-packages (from transformers==4.24.0->bnaug) (0.13.2)\n",
      "Requirement already satisfied: fsspec in /home/zubair/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0->bnaug) (2022.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/zubair/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0->bnaug) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/zubair/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.24.0->bnaug) (3.0.4)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->bnlp-toolkit==3.3.2->bnaug)\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/zubair/anaconda3/lib/python3.9/site-packages (from gensim->bnlp-toolkit==3.3.2->bnaug) (6.4.0)\n",
      "Requirement already satisfied: click in /home/zubair/anaconda3/lib/python3.9/site-packages (from nltk->bnlp-toolkit==3.3.2->bnaug) (8.1.3)\n",
      "Requirement already satisfied: joblib in /home/zubair/anaconda3/lib/python3.9/site-packages (from nltk->bnlp-toolkit==3.3.2->bnaug) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/zubair/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.24.0->bnaug) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zubair/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.24.0->bnaug) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/zubair/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.24.0->bnaug) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zubair/anaconda3/lib/python3.9/site-packages (from requests->transformers==4.24.0->bnaug) (2022.12.7)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/zubair/anaconda3/lib/python3.9/site-packages (from sklearn-crfsuite->bnlp-toolkit==3.3.2->bnaug) (0.9.8)\n",
      "Requirement already satisfied: six in /home/zubair/anaconda3/lib/python3.9/site-packages (from sklearn-crfsuite->bnlp-toolkit==3.3.2->bnaug) (1.16.0)\n",
      "Requirement already satisfied: tabulate in /home/zubair/anaconda3/lib/python3.9/site-packages (from sklearn-crfsuite->bnlp-toolkit==3.3.2->bnaug) (0.8.9)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=ba7a8f520a2868f7731c7c5711da06e2500ce953a4f13955c17d425c0b329dce\n",
      "  Stored in directory: /home/zubair/.cache/pip/wheels/fa/7a/e9/22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: wcwidth, emoji, ftfy, transformers, bnlp-toolkit, bnaug\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.30.2\n",
      "    Uninstalling transformers-4.30.2:\n",
      "      Successfully uninstalled transformers-4.30.2\n",
      "Successfully installed bnaug-1.1.2 bnlp-toolkit-3.3.2 emoji-1.7.0 ftfy-6.1.3 transformers-4.24.0 wcwidth-0.2.12\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bnaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05a33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['cls.predictions.decoder.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমরা ঢাকায বাস করি ।', 'আমি ঢাকায বাস করি ।', 'এখানে ঢাকায বাস করি ।', 'সেখানে ঢাকায বাস করি ।', 'বাসে ঢাকায বাস করি ।', 'আমি ঢাকায বাস করি ।', 'আমি ঢাকায বাস করি?', 'আমি ঢাকায বাস করি!', 'আমি ঢাকায বাস করি না', 'আমি ঢাকায বাস করি', 'আমি ঢাকায বাস করি ।', 'আমি ঢাকায বাস করছি ।', 'আমি ঢাকায বাস করেছি ।', 'আমি ঢাকায বাস করতাম ।', 'আমি ঢাকায বাস করিনি ।', 'আমি এখানে বাস করি ।', 'আমি সেখানে বাস করি ।', 'আমি বাস করি ।', 'আমি বাংলাদেশে বাস করি ।', 'আমি ওখানে বাস করি ।', 'আমি ঢাকায বাস করি ।', 'আমি ঢাকায বাস করি?', 'আমি ঢাকায বাস করি!', 'আমি ঢাকায বাস করি না', 'আমি ঢাকায বাস করি']\n"
     ]
    }
   ],
   "source": [
    "from bnaug.sentence import TokenReplacement\n",
    "\n",
    "tokr = TokenReplacement()\n",
    "text = \"আমি ঢাকায় বাস করি।\"\n",
    "output = tokr.masking_based(text, sen_n=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5718da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/bangla_word2vec/bnwiki_word2vec.model'\n",
      "[Errno 2] No such file or directory: '/bangla_word2vec/bnwiki_word2vec.model'\n",
      "[Errno 2] No such file or directory: '/bangla_word2vec/bnwiki_word2vec.model'\n",
      "[Errno 2] No such file or directory: '/bangla_word2vec/bnwiki_word2vec.model'\n",
      "[Errno 2] No such file or directory: '/bangla_word2vec/bnwiki_word2vec.model'\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bnaug.sentence import TokenReplacement\n",
    "\n",
    "tokr = TokenReplacement()\n",
    "text = \"আমি ঢাকায় বাস করি।\"\n",
    "model = \"/bangla_word2vec/bnwiki_word2vec.model\"\n",
    "output = tokr.word2vec_based(text, model=model, sen_n=5, word_n=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2662a5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'msc/bn_glove.300d.txt'\n",
      "[Errno 2] No such file or directory: 'msc/bn_glove.300d.txt'\n",
      "[Errno 2] No such file or directory: 'msc/bn_glove.300d.txt'\n",
      "[Errno 2] No such file or directory: 'msc/bn_glove.300d.txt'\n",
      "[Errno 2] No such file or directory: 'msc/bn_glove.300d.txt'\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from bnaug.sentence import TokenReplacement\n",
    "\n",
    "tokr = TokenReplacement()\n",
    "text = \"আমি ঢাকায় বাস করি।\"\n",
    "vector = \"msc/bn_glove.300d.txt\"\n",
    "output = tokr.glove_based(text, vector_path=vector, sen_n=5, word_n=5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f089f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnaug.sentence import BackTranslation\n",
    "\n",
    "bt = BackTranslation()\n",
    "text = \"বাংলা ভাষা আন্দোলন তদানীন্তন পূর্ব পাকিস্তানে সংঘটিত একটি সাংস্কৃতিক ও রাজনৈতিক আন্দোলন। \"\n",
    "output = bt.get_augmented_sentences(text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a298d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnaug.sentence import TextGeneration\n",
    "\n",
    "tg = TextGeneration()\n",
    "text = \"বিমানটি যখন মাটিতে নামার জন্য এয়ারপোর্টের কাছাকাছি আসছে, তখন ল্যান্ডিং গিয়ারের খোপের ঢাকনাটি খুলে যায়।\"\n",
    "output = tg.parapharse_generation(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnaug import randaug\n",
    "\n",
    "text = \"১০০ বাকি দিলাম\"\n",
    "output = randaug.remove_digits(text)\n",
    "print(output)\n",
    "\n",
    "text = \"১০০! বাকি দিলাম?\"\n",
    "output = randaug.remove_punctuations(text)\n",
    "print(output)\n",
    "\n",
    "text = \"আমি ১০০ বাকি দিলাম\"\n",
    "randaug.remove_stopwords(text)\n",
    "print(output)\n",
    "\n",
    "text = \"আমি ১০০ বাকি দিলাম\"\n",
    "randaug.remove_random_word(text)\n",
    "print(output)\n",
    "\n",
    "text = \"আমি ১০০ বাকি দিলাম\"\n",
    "randaug.remove_random_char(text)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
